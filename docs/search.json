[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "ğŸ“„ Resume",
    "section": "",
    "text": "ğŸ“„ Resume\nDownload my full CV:\n\nğŸ§‘â€ğŸ’¼ Professional Profile\nBusiness Analytics undergraduate with a passion for data-informed decision-making in elite sport, particularly football recruitment. Proficient in Python, SQL, Power BI, and workflow automation tools. Experienced across analytics, logistics, and administration roles. Committed to continuous learning and bridging the gap between technical solutions and strategic thinking.\nğŸ“ Education\nBachelorâ€™s in Business Studies (2021â€“2025), Dublin City University (DCU)\nModules include: Data Analytics, Workflow and Data Management, Machine Learning and Advanced Python, Econometrics, Business Strategy, and Digital Transformation.\nResearch Paper: Breakout Star Prediction using Machine Learning and SHAP-based interpretability in Professional Football.\nğŸ’¼ Experience\nBusiness Administration â€“ Analyse This (Aug 2023 â€“ May 2024)\n- Created custom online forms using Jotform\n- Managed LinkedIn marketing and client scheduling\n- General business administration\nCustomer Logistics Manager â€“ Infineon Technologies (Jan 2023 â€“ Jun 2023)\n- Coordinated product delivery\n- Optimized parts availability\n- Provided progress reports for global meetings\nShop Assistant â€“ Tesco (Jul 2019 â€“ Mar 2020)\n- Processed customer transactions\n- Managed stock and deliveries\n- Assisted with shop closing and audits\nBusiness Administration â€“ BAN Insurance (Various dates 2018â€“2019)\n- Claims reconciliation and validation\n- Reception and file organization\n- Client communication\nğŸ“ˆ Skills\nPython, SQL, Power BI, Git, Quarto, Jotform, VSCode, Jamovi, MS Word/Excel/PowerPoint, Zoom, MS Teams\nStrong written communication, teamwork, attention to detail, and fast typing.\nğŸ† Certifications\n- Machine Learning and Python (via DataCamp/Kubicle)\n- SQL for Business Analysis\nâš½ Interests\nData-driven sports analysis (Football, Basketball, NFL)\nPassion for analytics, strategy, and sustainable business"
  },
  {
    "objectID": "projects/project4.html",
    "href": "projects/project4.html",
    "title": "Project 4: Web Scraping & Visualization of Tech Salaries",
    "section": "",
    "text": "ğŸŒ Overview\nThis project demonstrates how to scrape real-world salary and job data from a public job listings website and visualize the results using Python. The goal was to identify patterns in tech salaries, job titles, and locations.\n\n\nğŸ§  Skills Demonstrated\n\nWeb scraping with BeautifulSoup\n\nHandling HTML elements and pagination\n\nCleaning and structuring scraped data with Pandas\n\nData storytelling with Matplotlib and Seaborn\n\nExtracting meaningful insights from unstructured web content\n\n\n\nğŸŒ Data Source\nData was scraped from a simulated job listings website that contained tech job posts across various global locations. Key fields extracted:\n\nJob Title\n\nSalary Estimate\n\nLocation\n\nCompany\n\nEmployment Type\n\n\n\nğŸ”§ Scraping & Data Prep (Python)\n\n\nShow the code\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\nurl = \"https://example.com/jobs\"  \nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, \"html.parser\")\n\njobs = []\nfor listing in soup.find_all(\"div\", class_=\"job-post\"):\n    title = listing.find(\"h2\").text\n    salary = listing.find(\"span\", class_=\"salary\").text\n    location = listing.find(\"span\", class_=\"location\").text\n    jobs.append({\"Title\": title, \"Salary\": salary, \"Location\": location})\n\ndf = pd.DataFrame(jobs)\ndf.head()\n\nimport matplotlib.pyplot as plt\n\nroles = ['Data Scientist', 'Software Engineer', 'Web Developer', 'Analyst']\nsalaries = [95000, 105000, 85000, 78000]\n\nplt.figure(figsize=(8, 4))\nplt.bar(roles, salaries, color=\"steelblue\")\nplt.title(\"Average Salary by Role\")\nplt.ylabel(\"Salary ($)\")\nplt.xticks(rotation=10)\nplt.tight_layout()\nplt.show()\n\nlocations = ['New York', 'San Francisco', 'London', 'Toronto']\njob_counts = [120, 90, 75, 60]\n\nplt.figure(figsize=(6, 4))\nplt.barh(locations, job_counts, color=\"mediumseagreen\")\nplt.title(\"Job Listings by Location\")\nplt.xlabel(\"Number of Listings\")\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ“ˆ Summary of Insights\n\nSoftware Engineers command the highest average salaries\n\nNew York and San Francisco are top hiring locations\n\nRoles like Analyst and Web Developer show lower average compensation\n\nWeb scraping enables access to up-to-date labor market trends"
  },
  {
    "objectID": "projects/project2.html",
    "href": "projects/project2.html",
    "title": "Project 2: Visualizing Luxury Auto Sales with Power BI",
    "section": "",
    "text": "This project explores the global sales performance of luxury automobiles between 2018 and 2020 using Power BI. It uncovers market trends, product line strengths, and customer behavior patterns. The dashboard is tailored for strategic decision-making in high-end automotive businesses."
  },
  {
    "objectID": "projects/project2.html#sales-per-year",
    "href": "projects/project2.html#sales-per-year",
    "title": "Project 2: Visualizing Luxury Auto Sales with Power BI",
    "section": "ğŸ“… Sales Per Year",
    "text": "ğŸ“… Sales Per Year\n\n\n\nSales Per Year"
  },
  {
    "objectID": "projects/project2.html#sales-per-product-line",
    "href": "projects/project2.html#sales-per-product-line",
    "title": "Project 2: Visualizing Luxury Auto Sales with Power BI",
    "section": "ğŸ·ï¸ Sales Per Product Line",
    "text": "ğŸ·ï¸ Sales Per Product Line\n\n\n\nSales Per Deal Size"
  },
  {
    "objectID": "projects/project2.html#illustrative-code-python-representation",
    "href": "projects/project2.html#illustrative-code-python-representation",
    "title": "Project 2: Visualizing Luxury Auto Sales with Power BI",
    "section": "ğŸ§ª Illustrative Code (Python Representation)",
    "text": "ğŸ§ª Illustrative Code (Python Representation)\n\n\nShow the visuals\nimport matplotlib.pyplot as plt\n\nproduct_lines = ['Classic Cars', 'Vintage Cars', 'Motorcycles', 'Trucks']\nsales_millions = [3.8, 2.4, 1.6, 1.2]\n\nplt.figure(figsize=(6, 4))\nplt.barh(product_lines, sales_millions, color=\"darkblue\")\nplt.xlabel(\"Sales (in millions USD)\")\nplt.title(\"Luxury Auto Sales by Product Line\")\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "ğŸš€ My Featured Projects",
    "section": "",
    "text": "âš½ Predicting Breakout Footballers\n\n\nSkills: Feature Engineering Classification SHAP Tools: Python XGBoost\n\n\nML pipeline that predicts which players will break out in the next season using 10M+ data points and SHAP explainability.\n\nView Project â”\n\n\n\n\n\n\n\nğŸš— Power BI Sales Dashboard\n\n\nSkills: Data Modeling Dashboarding DAX Tools: Power BI\n\n\nInteractive report analyzing luxury auto sales by year, region, and product line with KPIs and strategic visuals.\n\nView Project â”\n\n\n\n\n\n\n\nğŸ“Š SQL & Python Retail Analytics\n\n\nSkills: SQL Joins Subqueries Visualization Tools: SQLite Python Matplotlib\n\n\nRetail data insights using SQL and Python â€” uncovering trends in revenue, product lines, and customer regions with code-driven charts.\n\nView Project â”\n\n\n\n\n\n\n\nğŸ’¼ Web Scraping: Tech Salaries\n\n\nSkills: Web Scraping Visualization Python Tools: BeautifulSoup Pandas Matplotlib\n\n\nScraped tech job postings to visualize salary trends and identify location-based hiring patterns.\n\nView Project â”"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "ğŸ‘‹ About Me\nMy name is Conor English, and Iâ€™m a Business Studies student with a strong focus on analytics. Iâ€™m passionate about working as a data analyst in elite sports environments, particularly in player recruitment for professional teams in football, the NFL, or the NBA. I created this e-portfolio to showcase my capabilities across data analysis, machine learning, dashboarding, and SQL-driven insights, all applied through realistic, industry-inspired scenarios.\n\n\nğŸ“ Academic & Technical Background\nIâ€™m studying Business Studies at Dublin City University with a specialism in Analytics, where Iâ€™ve completed modules in machine learning, data management, and statistical modeling. In addition to my degree, Iâ€™ve earned multiple certifications in Python, Machine Learning, and SQL through DataCamp and Kubicle.\n\nğŸ§° Tool Proficiencies\nPython SQL Jupyter Power BI Excel Matplotlib Seaborn Git VS Code Quarto\n\n\n\nğŸ¯ Career Goals\nMy ultimate aim is to contribute to data-driven decision-making in professional sports, particularly focusing on identifying undervalued and emerging talent and supporting long-term recruitment strategies. Iâ€™m continually learning how to build models, manage large datasets, and extract actionable insights while also improving in areas like data cleaning, storytelling, exploratory analysis, and model validation.\n\n\nğŸ¤ Collaboration & Soft Skills\nThroughout my degree, Iâ€™ve participated in several collaborative, long-term group projects, including full-year assignments. These experiences have sharpened my abilities in:\n\nCommunication â€“ actively listening, contributing effectively in teams\n\nTeamwork â€“ navigating deadlines and sharing responsibilities\n\nProblem-Solving â€“ tackling ambiguity and applying critical thinking\n\n\n\n\n\n\n\nNote\n\n\n\nThese soft skills are just as important as technical tools. Working closely with peers has taught me to communicate clearly, resolve conflicts, and deliver quality work in a team setting essential for real-world analytics roles.\n\n\n\n\nğŸ’¬ Final Thought\n\nâ€œData will never replace intuitionâ€”but it can sharpen it.â€\nBlending instinct with insightâ€”for better decisions on and off the pitch."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ğŸ‘‹ Welcome",
    "section": "",
    "text": "Welcome to my professional e-portfolio, developed using Quarto, Git, and GitHub Pages. This site showcases my academic and technical development, particularly in the field of data analytics and player recruitment in elite-level sports.\n\n\n\nThis portfolio includes four end-to-end data projects, each highlighting specific technical and analytical skills across real-world scenarios:\n\nğŸ§  About: Learn more about my academic path, goals, and soft skills.\nğŸ“„ Resume: View my qualifications, certifications, and technical proficiencies.\nğŸš€ Projects: Explore machine learning models, dashboards, SQL analysis, and web scraping applications.\nğŸ” Reflection: Honest insights into my learning journey, challenges, and growth.\n\n\n\n\n\nğŸ Python ğŸ§® SQL ğŸ“Š Power BI ğŸ“˜ Jupyter ğŸ’» VS Code ğŸ”§ Git\n\n\n\n\nThrough these projects, Iâ€™ve explored tools like Python, SQL, Power BI, and BeautifulSoup, while building fluency with Git, VSCode, and collaborative project workflows. This site reflects both my technical foundation and a deepening commitment to a career in data analytics for sports performance and recruitment.\nğŸš€ Explore the Projects\n\n\n\n\n\n\n\n\nML pipeline to identify potential breakout stars in football using 10M+ datapoints and SHAP analysis.\n\nView Case Study â”\n\n\n\n\nâ€œSuccess is the sum of small efforts, repeated day in and day out.â€\nâ€” Robert Collier"
  },
  {
    "objectID": "index.html#what-youll-find-here",
    "href": "index.html#what-youll-find-here",
    "title": "ğŸ‘‹ Welcome",
    "section": "",
    "text": "This portfolio includes four end-to-end data projects, each highlighting specific technical and analytical skills across real-world scenarios:\n\nğŸ§  About: Learn more about my academic path, goals, and soft skills.\nğŸ“„ Resume: View my qualifications, certifications, and technical proficiencies.\nğŸš€ Projects: Explore machine learning models, dashboards, SQL analysis, and web scraping applications.\nğŸ” Reflection: Honest insights into my learning journey, challenges, and growth."
  },
  {
    "objectID": "index.html#tech-stack-highlights",
    "href": "index.html#tech-stack-highlights",
    "title": "ğŸ‘‹ Welcome",
    "section": "",
    "text": "ğŸ Python ğŸ§® SQL ğŸ“Š Power BI ğŸ“˜ Jupyter ğŸ’» VS Code ğŸ”§ Git"
  },
  {
    "objectID": "index.html#why-this-portfolio-matters",
    "href": "index.html#why-this-portfolio-matters",
    "title": "ğŸ‘‹ Welcome",
    "section": "",
    "text": "Through these projects, Iâ€™ve explored tools like Python, SQL, Power BI, and BeautifulSoup, while building fluency with Git, VSCode, and collaborative project workflows. This site reflects both my technical foundation and a deepening commitment to a career in data analytics for sports performance and recruitment.\nğŸš€ Explore the Projects"
  },
  {
    "objectID": "index.html#featured-project-predicting-breakout-footballers",
    "href": "index.html#featured-project-predicting-breakout-footballers",
    "title": "ğŸ‘‹ Welcome",
    "section": "",
    "text": "ML pipeline to identify potential breakout stars in football using 10M+ datapoints and SHAP analysis.\n\nView Case Study â”\n\n\n\n\nâ€œSuccess is the sum of small efforts, repeated day in and day out.â€\nâ€” Robert Collier"
  },
  {
    "objectID": "projects/project1.html",
    "href": "projects/project1.html",
    "title": "Project 1: Predicting Breakout Footballers with Machine Learning",
    "section": "",
    "text": "In this project, I built a machine learning pipeline to predict breakout football playersâ€”those expected to drastically improve their goals or assists in the coming season. This was done using real-world player statistics and advanced ML interpretability tools like SHAP.\nThe project used over 10 million data points and statistics from 2013 to April 2025, and was built to support recruitment workflows like those used by clubs such as Brighton or Liverpool."
  },
  {
    "objectID": "projects/project1.html#explanation-of-algorithm",
    "href": "projects/project1.html#explanation-of-algorithm",
    "title": "Project 1: Predicting Breakout Footballers with Machine Learning",
    "section": "ğŸ¬ Explanation of Algorithm",
    "text": "ğŸ¬ Explanation of Algorithm"
  },
  {
    "objectID": "projects/project3.html",
    "href": "projects/project3.html",
    "title": "Project 3: SQL and Python for Retail Analytics",
    "section": "",
    "text": "ğŸ›’ Overview\nThis project demonstrates how to analyze retail sales data using SQL queries and Python visualizations. It simulates a real-world scenario where a data analyst uses a database of customer transactions, product lines, and revenue to uncover insights.\n\n\nğŸ§  Skills Demonstrated\n\nSQL querying and JOIN operations\n\nData extraction and transformation\n\nSubqueries and window functions\n\nPython-based data visualization (Matplotlib & Seaborn)\n\nExploratory data storytelling\n\n\n\nğŸ—ƒï¸ Database Structure\nThe project used a mock retail database with the following tables: - orders, products, customers, sales, and dates\n\n\nğŸ” Key SQL Queries\nSELECT product_line, SUM(total_amount) AS total_sales\nFROM sales\nGROUP BY product_line\nORDER BY total_sales DESC;\nSELECT country, ROUND(SUM(total_amount), 2) AS revenue\nFROM customers\nJOIN sales ON customers.customer_id = sales.customer_id\nGROUP BY country\nORDER BY revenue DESC\nLIMIT 5;\nSELECT strftime('%Y-%m', order_date) AS month, SUM(total_amount) AS revenue\nFROM sales\nGROUP BY month\nORDER BY month;\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']\nrevenue = [12000, 14500, 16000, 14000, 17500, 19000]\n\nplt.figure(figsize=(8, 4))\nplt.plot(months, revenue, marker='o', color='blue')\nplt.title(\"Monthly Revenue Trend\")\nplt.xlabel(\"Month\")\nplt.ylabel(\"Revenue ($)\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nlines = ['Classic Cars', 'Motorcycles', 'Planes', 'Vintage Cars']\nsales = [380000, 140000, 90000, 105000]\n\nplt.figure(figsize=(6, 4))\nplt.barh(lines, sales, color='green')\nplt.title(\"Sales by Product Line\")\nplt.xlabel(\"Total Revenue\")\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nğŸ“ˆ Summary of Insights\n\nClassic Cars lead all categories in total sales\n\nRevenue peaks in May and June, showing seasonality\n\nTop countries by revenue: USA, Germany, UK\n\nMonthly line chart illustrates revenue growth patterns\n\nProduct line bar chart highlights key market segments"
  },
  {
    "objectID": "reflection.html",
    "href": "reflection.html",
    "title": "ğŸ§  Reflection",
    "section": "",
    "text": "ğŸ” Why Reflection Matters\nReflection is one of the most powerful tools for long-term growth both personally and professionally. By looking back honestly at what went well, what could be improved, and what was learned, we create a feedback loop that sharpens our thinking and strengthens our skills. Being real with yourself is never easy, but itâ€™s where meaningful progress begins. This page is an attempt to do just that: to step back and assess not just what I built, but what I learned in the process.\n\n\n\nâš½ Project 1: Predicting Breakout Footballers with Machine Learning\nWhat I learned:\nThis project taught me how to build and evaluate a complete machine learning pipeline from preprocessing and feature engineering to model selection and SHAP explainability. It showed me how complex real-world data can be and the importance of domain-specific labels.\nWhat went well / what didnâ€™t:\nThe modeling stage and SHAP integration worked very smoothly, but cleaning the data and designing useful breakout labels were much more difficult than expected. It taught me that thoughtful definitions matter as much as model performance.\nSkills improved:\nFeature engineering Model evaluation XGBoost SHAP explainability Sports analytics\n\n\n\nğŸ“Š Project 2: Power BI Sales Dashboard\nWhat I learned:\nThis project deepened my understanding of how to transform raw business data into clear, actionable visual insights. It was my first time using Power BI at this scale.\nWhat went well / what didnâ€™t:\nThe visuals and KPIs came together well, and I was especially proud of the product line analysis. However, connecting all filters correctly and managing relationships took longer than expected.\nSkills improved:\nDAX Power BI dashboarding Visual storytelling KPI tracking\n\n\n\nğŸ›’ Project 3: SQL and Python for Retail Analytics\nWhat I learned:\nI learned how to structure SQL queries to answer real business questions and integrate that analysis into Python for visual output.\nWhat went well / what didnâ€™t:\nWriting efficient SQL and connecting Python visualizations worked nicely. If I were to do it again, I would make the data more realistic and varied to simulate a larger database as this would provide more accurate results\nSkills improved:\nSQL joins Grouping and aggregation Matplotlib Python-based visual analytics\n\n\n\nğŸŒ Project 4: Web Scraping & Visualization of Tech Salaries\nWhat I learned:\nThis was my first time using BeautifulSoup to scrape websites. It showed me how data doesnâ€™t always come clean and structured and how to handle it when it doesnâ€™t.\nWhat went well / what didnâ€™t:\nI was happy with the scraping logic and final plots. Some elements of HTML parsing were challenging and required trial-and-error and patience.\nSkills improved:\nBeautifulSoup scraping HTML parsing Seaborn Matplotlib\n\n\n\nğŸš€ Final Reflection\nLooking back, this portfolio has done more than just help me pass a module it has cemented my confidence in this career path. I began the year interested in data analysis and player recruitment. Now, Iâ€™m certain itâ€™s what I want to do.\nMy understanding of the tools, techniques, and mindset required has grown dramatically. I now know that good analysts arenâ€™t just technical they think critically, communicate clearly, and reflect honestly. Working in groups on multiple assignments, especially year-long ones, has helped me develop soft skills like\nCommunication Teamwork Problem-solving\nâ€” the same ones that matter in real-world teams.\n\n\n\n\n\n\nBranching Decision\n\n\n\nWhile I initially intended to use multiple Git branches to separate distinct features of the portfolio, I encountered significant integration conflicts and time constraints while managing interdependent changes across pages. To maintain the integrity and consistency of the site, I made the decision to keep development on a unified branch. This ensured stability and allowed me to focus more on analysis, design polish, and storytellingâ€”core goals of this portfolio. Each commit has been carefully documented for transparency and traceability.\n\n\n\nâ€œGrowth begins the moment we choose honesty over ego, and reflection over routine.â€"
  }
]